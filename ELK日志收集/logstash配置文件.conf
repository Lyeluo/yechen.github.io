#由log4j2打印日志到kafka队列
#然后logstash在kafka上接收日志消息
#再由logstash推送日志消息到elasticsearch
# elasticsearch、logstash版本均为6.2.3
#接收kafka上的日志消息
input{
    kafka {
        bootstrap_servers => ["192.168.2.183:9092,192.168.2.184:9092,192.168.2.185:9092"]
        group_id => "ecs2"
        auto_offset_reset => "earliest"
        consumer_threads => "5"
        #这里不设置为true得话，下面获取不到值%{[@metadata][kafka][topic]}
        decorate_events => "true"
        topics => ["ms-job-console","log4j"]
        type => "ecs2"
   }
}
#数据过滤
filter {
   if [type] == 'ecs2'{
     grok {
        match => { "message" => "%{GREEDYDATA:date} %{GREEDYDATA:timer} %{LOGLEVEL:loglevel} %{GREEDYDATA:service}  %{GREEDYDATA:className} \[Class = %{GREEDYDATA:className1}\] \[File = %{GREEDYDATA:classFile}\] \[Line = %{NUMBER:classLine}\] \[Method = %{GREEDYDATA:classMethod}\] \[%{GREEDYDATA:logModule}\] %{GREEDYDATA:log-context}"}
     }
   }
}
#输出配置为本机的9200端口，这是ElasticSerach服务的监听端口
output {
   if "ecs2" in [type]{
     elasticsearch {
       hosts => ["192.168.2.186:9200"]
       #%{[@metadata][kafka][topic]} 为kafka得topic名称
       index=>"%{[@metadata][kafka][topic]}-%{+YYYY.MM.dd}"
     }
  }
}
